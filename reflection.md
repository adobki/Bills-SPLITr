# How AI impacted the build process of **Bills SPLITr**

I built Bills SPLITr with AI as my primary development partner, and the project reshaped how I design, code, and document work. Almost all of the application code and about ninety percent of the documentation were generated iteratively by AI from carefully designed prompts. Documentation here includes the project README and the rule file I saved as `project-spec.mdc` for Cursor and `.voidrules` for Void. I used ChatGPT (GPT-5 in the web app) to kick off high-level brainstorming, define wireframes and user flows, draft the initial database schema, etc.

My toolchain was intentionally free: I relied exclusively on no-cost tiers and available agent IDEs. I used two IDE/agent environments, Cursor and Void. Cursor became my primary environment because its agent and UI felt more polished and suited to larger generation tasks and orchestration. However, Cursor’s free tier limited me to only a few prompts per day and restricted me to GPT-4.1; the Auto option and access to other models required a paid upgrade. I reached that free-usage ceiling earlier, while learning Cursor as part of this course, so I learned to ration prompts and prioritize high-value requests.

To conserve Cursor prompts and keep momentum, I used Void for quick fixes and small edits. Whenever a trivial bug or immediate correction was required I asked Void to patch the code, which preserved my Cursor balance for orchestration and larger updates. For less urgent or non-breaking changes, I aggregated issues and bundled them into the next structured prompt to Cursor, asking it to perform fixes and then proceed to the next phase in the development plan. This hybrid workflow — Cursor for heavy lifting and Void for nimble edits — kept iterations flowing despite tooling constraints.

What worked exceptionally well was the speed of scaffolding and the consistency of generated outputs. Using ChatGPT for planning produced a coherent README and formalized rules that functioned as a single source of truth. The agents generated repeatable component patterns, wiring logic, and even starter test stubs, enabling me - *a developer with no prior frontend or React experience* - to assemble a working interface quickly: in one day. I adopted an incremental, test-each-step approach: implement a minimal feature, run it, observe behavior, and require the agent to fix observable failures before adding complexity. That practice kept bugs contained and progress steady.

The most limiting aspects were quotas and occasional incorrect assumptions in generated code. AI sometimes guessed data shapes or edge-case behavior, so I developed the habit of reviewing output like a human code reviewer: check types, boundaries, and error handling; specify acceptance criteria; and embed small verifiable examples in prompts. Free-tier constraints forced me to write fewer, higher-quality prompts and to include concrete tests or examples that the agent could use to validate its output.

In summary, AI amplified my productivity and accelerated learning while teaching restraint. Careful prompting, iterative testing, and a dual-agent strategy allowed me to transform an unfamiliar stack into a polished prototype. Going forward, I will document effective prompt patterns, build reusable templates, and add automated checks to make future AI-driven development faster and more reliable; and I will share these insights with my peers.
